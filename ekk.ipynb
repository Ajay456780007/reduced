{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095d29d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dataset2/dna_sequence/Spenn_v2.0-HC_cds.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Replace with your file path\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m fasta_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_fasta_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/dataset2/dna_sequence/Spenn_v2.0-HC_cds.fasta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or \"your_file.f\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mread_fasta_to_dataframe\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m open_func \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mopen \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[0;32m      8\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mopen_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m SeqIO\u001b[38;5;241m.\u001b[39mparse(handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m         records\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m: record\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: record\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(record\u001b[38;5;241m.\u001b[39mseq)\n\u001b[0;32m     15\u001b[0m         })\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataset2/dna_sequence/Spenn_v2.0-HC_cds.fasta'"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "def read_fasta_to_dataframe(file_path):\n",
    "    open_func = gzip.open if file_path.endswith('.gz') else open\n",
    "\n",
    "    records = []\n",
    "    with open_func(file_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            records.append({\n",
    "                \"ID\": record.id,\n",
    "                \"Description\": record.description,\n",
    "                \"Sequence\": str(record.seq)\n",
    "            })\n",
    "\n",
    "    # Load into DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Replace with your file path\n",
    "fasta_df = read_fasta_to_dataframe(\"data/dataset2/dna_sequence/Spenn_v2.0-HC_cds.fasta\")  # or \"your_file.f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4999ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sopen00g001010.1</td>\n",
       "      <td>Sopen00g001010.1 | hypothetical protein</td>\n",
       "      <td>ATGCGGGTCTATGAGACTGTATGGCGACGGTATGGTGTAATTGCGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sopen00g001030.1</td>\n",
       "      <td>Sopen00g001030.1 | Cytochrome c oxidase subuni...</td>\n",
       "      <td>ATGATTGAATCTCAGAGGCATTCTTATCATTTGGTAGATCCAAGTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sopen00g001040.1</td>\n",
       "      <td>Sopen00g001040.1 | ATP synthase alpha/beta fam...</td>\n",
       "      <td>ATGGAACTTTCTCCCCGAGCTGCGGAACTAACAAGTCTATTAGAAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sopen00g001050.1</td>\n",
       "      <td>Sopen00g001050.1 | Mitochondrial ATP synthase ...</td>\n",
       "      <td>ATGCCCAATAAAAGTAAAAGACCCTTCTCTTCTATTTTTAAAGGAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sopen00g001060.1</td>\n",
       "      <td>Sopen00g001060.1 | hypothetical protein</td>\n",
       "      <td>ATGATTTGTTTTATGGACAGTTATATTATTTCCAGCCCAAATGAGG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                        Description  \\\n",
       "0  Sopen00g001010.1            Sopen00g001010.1 | hypothetical protein   \n",
       "1  Sopen00g001030.1  Sopen00g001030.1 | Cytochrome c oxidase subuni...   \n",
       "2  Sopen00g001040.1  Sopen00g001040.1 | ATP synthase alpha/beta fam...   \n",
       "3  Sopen00g001050.1  Sopen00g001050.1 | Mitochondrial ATP synthase ...   \n",
       "4  Sopen00g001060.1            Sopen00g001060.1 | hypothetical protein   \n",
       "\n",
       "                                            Sequence  \n",
       "0  ATGCGGGTCTATGAGACTGTATGGCGACGGTATGGTGTAATTGCGG...  \n",
       "1  ATGATTGAATCTCAGAGGCATTCTTATCATTTGGTAGATCCAAGTC...  \n",
       "2  ATGGAACTTTCTCCCCGAGCTGCGGAACTAACAAGTCTATTAGAAA...  \n",
       "3  ATGCCCAATAAAAGTAAAAGACCCTTCTCTTCTATTTTTAAAGGAA...  \n",
       "4  ATGATTTGTTTTATGGACAGTTATATTATTTCCAGCCCAAATGAGG...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_df.head() # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        seqid    source    type start   end score strand phase  \\\n",
      "0  Spenn-ch00  AUGUSTUS    gene  5691  9072  0.84      -     .   \n",
      "1  Spenn-ch00  AUGUSTUS    mRNA  5691  9072  0.84      -     .   \n",
      "2  Spenn-ch00  AUGUSTUS    exon  5691  5899     .      -     .   \n",
      "3  Spenn-ch00  AUGUSTUS     CDS  5723  5899     1      -     0   \n",
      "4  Spenn-ch00  AUGUSTUS  intron  5900  8915     1      -     .   \n",
      "\n",
      "                                          attributes  \n",
      "0             ID=Sopen00g001010;Name=Sopen00g001010;  \n",
      "1  ID=Sopen00g001010.1;Name=Sopen00g001010.1;Pare...  \n",
      "2  ID=exon:Sopen00g001010.1.1;Parent=Sopen00g0010...  \n",
      "3  ID=cds:Sopen00g001010.1.1;Parent=Sopen00g00101...  \n",
      "4  ID=intron:Sopen00g001010.1.1;Parent=Sopen00g00...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Column names for GFF3\n",
    "col_names = [\n",
    "    \"seqid\", \"source\", \"type\", \"start\", \"end\", \n",
    "    \"score\", \"strand\", \"phase\", \"attributes\"\n",
    "]\n",
    "\n",
    "# Read GFF3 file, skipping comment lines\n",
    "gff_df = pd.read_csv(\n",
    "    \"data/dataset2/gff3_files/Spenn_v2.0-HC_gene_models.gff\",\n",
    "    sep=\"\\t\",\n",
    "    comment='#',\n",
    "    names=col_names,\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "# Show first few rows\n",
    "print(gff_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpm=pd.read_csv(\"data/dataset2/tpm/Supplementary Table5_ TPM Data of Solanum pennellii_Solanum arcanum and Solanum lycopersicum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene ID</th>\n",
       "      <th>SRR12002099</th>\n",
       "      <th>SRR12002100</th>\n",
       "      <th>SRR12002101</th>\n",
       "      <th>SRR12002102</th>\n",
       "      <th>SRR12002103</th>\n",
       "      <th>SRR12002104</th>\n",
       "      <th>SRR12002105</th>\n",
       "      <th>SRR12002106</th>\n",
       "      <th>SRR12002107</th>\n",
       "      <th>...</th>\n",
       "      <th>SRR5171929</th>\n",
       "      <th>SRR5171930</th>\n",
       "      <th>SRR5171931</th>\n",
       "      <th>SRR5171933</th>\n",
       "      <th>SRR5171934</th>\n",
       "      <th>SRR5171935</th>\n",
       "      <th>SRR5171936</th>\n",
       "      <th>SRR6762716</th>\n",
       "      <th>SRR6762717</th>\n",
       "      <th>SRR6762718</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solyc00g005000.3</td>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950161</td>\n",
       "      <td>0.367369</td>\n",
       "      <td>0.345971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Solyc00g005040.3</td>\n",
       "      <td>0.739067</td>\n",
       "      <td>0.168003</td>\n",
       "      <td>1.098245</td>\n",
       "      <td>0.077184</td>\n",
       "      <td>0.863073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266766</td>\n",
       "      <td>1.012316</td>\n",
       "      <td>1.174495</td>\n",
       "      <td>2.560131</td>\n",
       "      <td>1.651347</td>\n",
       "      <td>1.102297</td>\n",
       "      <td>1.322495</td>\n",
       "      <td>0.736884</td>\n",
       "      <td>0.752721</td>\n",
       "      <td>0.705994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Solyc00g005050.3</td>\n",
       "      <td>88.353252</td>\n",
       "      <td>51.370714</td>\n",
       "      <td>85.750375</td>\n",
       "      <td>135.155023</td>\n",
       "      <td>105.785519</td>\n",
       "      <td>104.378816</td>\n",
       "      <td>101.558330</td>\n",
       "      <td>90.407801</td>\n",
       "      <td>76.934708</td>\n",
       "      <td>...</td>\n",
       "      <td>52.783637</td>\n",
       "      <td>46.519283</td>\n",
       "      <td>46.956786</td>\n",
       "      <td>39.789561</td>\n",
       "      <td>36.562843</td>\n",
       "      <td>36.405200</td>\n",
       "      <td>36.569208</td>\n",
       "      <td>91.656989</td>\n",
       "      <td>70.302637</td>\n",
       "      <td>90.986289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Solyc00g005060.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solyc00g005080.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.072193</td>\n",
       "      <td>6.487029</td>\n",
       "      <td>0.637166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.018035</td>\n",
       "      <td>5.827764</td>\n",
       "      <td>0.848280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675691</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.525263</td>\n",
       "      <td>0.253184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Gene ID  SRR12002099  SRR12002100  SRR12002101  SRR12002102  \\\n",
       "0  Solyc00g005000.3     0.489835     0.000000     0.950161     0.367369   \n",
       "1  Solyc00g005040.3     0.739067     0.168003     1.098245     0.077184   \n",
       "2  Solyc00g005050.3    88.353252    51.370714    85.750375   135.155023   \n",
       "3  Solyc00g005060.1     0.000000     0.000000     0.000000     0.000000   \n",
       "4  Solyc00g005080.2     0.000000     2.072193     6.487029     0.637166   \n",
       "\n",
       "   SRR12002103  SRR12002104  SRR12002105  SRR12002106  SRR12002107  ...  \\\n",
       "0     0.345971     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "1     0.863073     0.000000     0.000000     0.000000     0.210530  ...   \n",
       "2   105.785519   104.378816   101.558330    90.407801    76.934708  ...   \n",
       "3     0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "4     0.000000     0.000000     5.018035     5.827764     0.848280  ...   \n",
       "\n",
       "   SRR5171929  SRR5171930  SRR5171931  SRR5171933  SRR5171934  SRR5171935  \\\n",
       "0    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "1    0.266766    1.012316    1.174495    2.560131    1.651347    1.102297   \n",
       "2   52.783637   46.519283   46.956786   39.789561   36.562843   36.405200   \n",
       "3    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4    0.000000    0.000000    0.000000    0.000000    0.505354    0.000000   \n",
       "\n",
       "   SRR5171936  SRR6762716  SRR6762717  SRR6762718  \n",
       "0    0.000000    0.000000    0.007949    0.000000  \n",
       "1    1.322495    0.736884    0.752721    0.705994  \n",
       "2   36.569208   91.656989   70.302637   90.986289  \n",
       "3    0.000000    0.000000    0.000000    0.000000  \n",
       "4    0.675691    0.608108    0.525263    0.253184  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea64942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "dna_sequence_path=\"dataset/dataset1/dna_chromosomes/\"\n",
    "gff3_path=\"dataset/dataset1/gff3_files/\"\n",
    "\n",
    "fasta_files = sorted([\n",
    "     os.path.join(dna_sequence_path, f) for f in os.listdir(dna_sequence_path)\n",
    "     if f.lower().endswith(\".fa.gz\")\n",
    " ])\n",
    "gff3_files= sorted([\n",
    "     os.path.join(gff3_path, f) for f in os.listdir(gff3_path)\n",
    "     if f.lower().endswith(\".gff3\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts hereeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a11b9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.1.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.1.gff3\n",
      "Found 10967 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.1.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.10.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.10.gff3\n",
      "Found 4918 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.10.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.2.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.2.gff3\n",
      "Found 8537 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.2.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.3.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.3.gff3\n",
      "Found 7734 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.3.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.4.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.4.gff3\n",
      "Found 7593 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.4.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.5.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.5.gff3\n",
      "Found 8224 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.5.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.6.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.6.gff3\n",
      "Found 6075 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.6.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.7.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.7.gff3\n",
      "Found 5628 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.7.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.8.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.8.gff3\n",
      "Found 6668 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.8.gff3\n",
      "Processing: Zea_mays.Zm-B73-REFERENCE-NAM-5.0.dna.chromosome.9.fa.gz with Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.9.gff3\n",
      "Found 5447 CDS transcripts in Zea_mays.Zm-B73-REFERENCE-NAM-5.0.60.chromosome.9.gff3\n",
      "                     transcript_id chrom strand  start    end  \\\n",
      "0  transcript:Zm00001eb000010_T001     1      +  34721  38366   \n",
      "1  transcript:Zm00001eb000020_T004     1      -  41526  43900   \n",
      "2  transcript:Zm00001eb000020_T001     1      -  41526  45913   \n",
      "3  transcript:Zm00001eb000020_T002     1      -  42239  45913   \n",
      "4  transcript:Zm00001eb000020_T003     1      -  43761  45913   \n",
      "\n",
      "                                            sequence  \n",
      "0  ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...  \n",
      "1  TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...  \n",
      "2  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n",
      "3  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n",
      "4  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "from urllib.parse import unquote\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths ===\n",
    "dna_dir = \"dataset/dataset1/dna_chromosomes/\"\n",
    "gff3_dir = \"dataset/dataset1/gff3_files/\"\n",
    "\n",
    "# === Collect all FASTA and GFF3 files ===\n",
    "fasta_files = sorted([\n",
    "    os.path.join(dna_dir, f) for f in os.listdir(dna_dir)\n",
    "    if f.lower().endswith(\".fa.gz\")\n",
    "])\n",
    "gff3_files = sorted([\n",
    "    os.path.join(gff3_dir, f) for f in os.listdir(gff3_dir)\n",
    "    if f.lower().endswith(\".gff3\")\n",
    "])\n",
    "\n",
    "# === Parse GFF3 attributes ===\n",
    "def parse_attributes(attr_str):\n",
    "    attr_dict = {}\n",
    "    for pair in attr_str.strip().split(\";\"):\n",
    "        if \"=\" in pair:\n",
    "            key, value = pair.split(\"=\", 1)\n",
    "            attr_dict[key.strip()] = unquote(value.strip())\n",
    "    return attr_dict\n",
    "\n",
    "# === Function to parse GFF3 and extract CDS entries for a given chromosome ===\n",
    "def parse_gff3_cds(gff3_file, chrom_id):\n",
    "    cds_dict = {}\n",
    "    with open(gff3_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            if parts[2] != \"CDS\":\n",
    "                continue\n",
    "            if parts[0] != chrom_id:\n",
    "                continue\n",
    "\n",
    "            start = int(parts[3]) - 1  # GFF3 is 1-based; convert to 0-based\n",
    "            end = int(parts[4])        # end is exclusive\n",
    "            strand = parts[6]\n",
    "            attrs = parse_attributes(parts[8])\n",
    "            parent_id = attrs.get(\"Parent\", \"NA\")\n",
    "\n",
    "            if parent_id not in cds_dict:\n",
    "                cds_dict[parent_id] = {\n",
    "                    \"strand\": strand,\n",
    "                    \"ranges\": []\n",
    "                }\n",
    "            cds_dict[parent_id][\"ranges\"].append((start, end))\n",
    "    return cds_dict\n",
    "\n",
    "# === Extract CDS sequences with start/end positions ===\n",
    "cds_sequences = []\n",
    "\n",
    "for fasta_path, gff_path in zip(fasta_files, gff3_files):\n",
    "    print(f\"Processing: {os.path.basename(fasta_path)} with {os.path.basename(gff_path)}\")\n",
    "\n",
    "    # Read chromosome sequence\n",
    "    if fasta_path.endswith(\".gz\"):\n",
    "        with gzip.open(fasta_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            record = next(SeqIO.parse(f, \"fasta\"))\n",
    "    else:\n",
    "        with open(fasta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            record = next(SeqIO.parse(f, \"fasta\"))\n",
    "\n",
    "    chrom_seq = record.seq\n",
    "    chrom_id = record.id\n",
    "\n",
    "    # Parse CDS entries\n",
    "    cds_dict = parse_gff3_cds(gff_path, chrom_id)\n",
    "    print(f\"Found {len(cds_dict)} CDS transcripts in {os.path.basename(gff_path)}\")\n",
    "\n",
    "    for parent_id, info in cds_dict.items():\n",
    "        strand = info[\"strand\"]\n",
    "        regions = sorted(info[\"ranges\"], key=lambda x: x[0])  # sort by start\n",
    "        full_seq = \"\".join(str(chrom_seq[start:end]) for start, end in regions)\n",
    "        if strand == \"-\":\n",
    "            full_seq = str(Seq(full_seq).reverse_complement())\n",
    "\n",
    "        # Get the genomic span\n",
    "        transcript_start = min(start for start, end in regions)\n",
    "        transcript_end = max(end for start, end in regions)\n",
    "\n",
    "        cds_sequences.append({\n",
    "            \"transcript_id\": parent_id,\n",
    "            \"chrom\": chrom_id,\n",
    "            \"strand\": strand,\n",
    "            \"start\": transcript_start,\n",
    "            \"end\": transcript_end,\n",
    "            \"sequence\": full_seq\n",
    "        })\n",
    "\n",
    "# === Convert to DataFrame and show\n",
    "df_cds = pd.DataFrame(cds_sequences)\n",
    "print(df_cds.head())\n",
    "\n",
    "# Save if needed\n",
    "# df_cds.to_csv(\"parsed_cds_with_positions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817acc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(df_cds['sequence'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635102b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>chrom</th>\n",
       "      <th>strand</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript:Zm00001eb000010_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>34721</td>\n",
       "      <td>38366</td>\n",
       "      <td>ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcript:Zm00001eb000020_T004</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>43900</td>\n",
       "      <td>TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcript:Zm00001eb000020_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcript:Zm00001eb000020_T002</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>42239</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcript:Zm00001eb000020_T003</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>43761</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transcript:Zm00001eb000050_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>108553</td>\n",
       "      <td>109927</td>\n",
       "      <td>ATGCCATTTCATGAGGTCGTCCAACAACCTCATAAGACGTTTGTAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transcript:Zm00001eb000060_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>188922</td>\n",
       "      <td>189282</td>\n",
       "      <td>ATGGGGCTGAAGCGGCCTACGGCGGGAGCGGGGGAGGCAGCAGCGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transcript:Zm00001eb000070_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>190191</td>\n",
       "      <td>198832</td>\n",
       "      <td>ATGAAGAGAAAGGAAAACTCCGCGCACTCGGCGTCGCCTCTCAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transcript:Zm00001eb000080_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>201176</td>\n",
       "      <td>202121</td>\n",
       "      <td>ATGAAGACAGGGATCGTTGAGAGTGATCTGGTTCTAACAGTTAGTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transcript:Zm00001eb000100_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>207555</td>\n",
       "      <td>209641</td>\n",
       "      <td>ATGCTTCACTCCCAGTATGCCTCGCAGACGTCCAAACAGAACCCGG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     transcript_id chrom strand   start     end  \\\n",
       "0  transcript:Zm00001eb000010_T001     1      +   34721   38366   \n",
       "1  transcript:Zm00001eb000020_T004     1      -   41526   43900   \n",
       "2  transcript:Zm00001eb000020_T001     1      -   41526   45913   \n",
       "3  transcript:Zm00001eb000020_T002     1      -   42239   45913   \n",
       "4  transcript:Zm00001eb000020_T003     1      -   43761   45913   \n",
       "5  transcript:Zm00001eb000050_T001     1      -  108553  109927   \n",
       "6  transcript:Zm00001eb000060_T001     1      -  188922  189282   \n",
       "7  transcript:Zm00001eb000070_T001     1      -  190191  198832   \n",
       "8  transcript:Zm00001eb000080_T001     1      -  201176  202121   \n",
       "9  transcript:Zm00001eb000100_T001     1      -  207555  209641   \n",
       "\n",
       "                                            sequence  \n",
       "0  ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...  \n",
       "1  TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...  \n",
       "2  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n",
       "3  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n",
       "4  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  \n",
       "5  ATGCCATTTCATGAGGTCGTCCAACAACCTCATAAGACGTTTGTAG...  \n",
       "6  ATGGGGCTGAAGCGGCCTACGGCGGGAGCGGGGGAGGCAGCAGCGC...  \n",
       "7  ATGAAGAGAAAGGAAAACTCCGCGCACTCGGCGTCGCCTCTCAACA...  \n",
       "8  ATGAAGACAGGGATCGTTGAGAGTGATCTGGTTCTAACAGTTAGTC...  \n",
       "9  ATGCTTCACTCCCAGTATGCCTCGCAGACGTCCAAACAGAACCCGG...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0219a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cds.to_csv(\"dataset/dataset1/cds_sequences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "431b72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     transcript_id  chrom strand  start    end  \\\n",
      "0  transcript:Zm00001eb000010_T001      1      +  34721  38366   \n",
      "1  transcript:Zm00001eb000020_T004      1      -  41526  43900   \n",
      "2  transcript:Zm00001eb000020_T001      1      -  41526  45913   \n",
      "3  transcript:Zm00001eb000020_T002      1      -  42239  45913   \n",
      "4  transcript:Zm00001eb000020_T003      1      -  43761  45913   \n",
      "\n",
      "                                            sequence            d_id   avg_tpm  \n",
      "0  ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...  Zm00001d027230  2.477070  \n",
      "1  TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...  Zm00001d027231  7.861029  \n",
      "2  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  \n",
      "3  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  \n",
      "4  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Load your data\n",
    "df_cds = pd.read_csv(\"dataset/dataset1/cds_sequences.csv\")\n",
    "alias_df = pd.read_csv(\"dataset/dataset1/alias/genes_to_alias_ids.tsv\", sep=\"\\t\", header=None)\n",
    "tpm_df = pd.read_csv(\"dataset/dataset1/geo_file/abundance.tsv\", sep=\"\\t\")\n",
    "\n",
    "# === 2. Clean column names\n",
    "alias_df.columns = [\"e_id\", \"source\", \"d_id\", \"agpv4_id\"]\n",
    "tpm_df[\"gene_id\"] = tpm_df[\"target_id\"].apply(lambda x: x.split(\"_T\")[0])\n",
    "\n",
    "# === 3. Clean `transcript_id` in `df_cds`\n",
    "df_cds[\"clean_gene_id\"] = df_cds[\"transcript_id\"].apply(\n",
    "    lambda x: x.replace(\"transcript:\", \"\").split(\"_T\")[0]\n",
    ")\n",
    "\n",
    "# === 4. Map Zm00001eb... to Zm00001d... via alias file\n",
    "df_merged = df_cds.merge(\n",
    "    alias_df[[\"e_id\", \"d_id\"]],\n",
    "    left_on=\"clean_gene_id\",\n",
    "    right_on=\"e_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 5. Average TPM per gene (Zm00001d...)\n",
    "avg_tpm = (\n",
    "    tpm_df.groupby(\"gene_id\", as_index=False)[\"tpm\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"tpm\": \"avg_tpm\"})\n",
    ")\n",
    "\n",
    "# === 6. Merge with averaged TPM values\n",
    "final_df = df_merged.merge(\n",
    "    avg_tpm,\n",
    "    left_on=\"d_id\",\n",
    "    right_on=\"gene_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 7. Drop intermediate columns\n",
    "final_df = final_df.drop(columns=[\"clean_gene_id\", \"e_id\", \"gene_id\"])\n",
    "\n",
    "# === 8. Output final result\n",
    "print(final_df.head())\n",
    "# Optional: Save to CSV\n",
    "# final_df.to_csv(\"final_matched_cds_avg_tpm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "194b5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>chrom</th>\n",
       "      <th>strand</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>d_id</th>\n",
       "      <th>avg_tpm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript:Zm00001eb000010_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>34721</td>\n",
       "      <td>38366</td>\n",
       "      <td>ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...</td>\n",
       "      <td>Zm00001d027230</td>\n",
       "      <td>2.477070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcript:Zm00001eb000020_T004</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>43900</td>\n",
       "      <td>TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcript:Zm00001eb000020_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcript:Zm00001eb000020_T002</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>42239</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcript:Zm00001eb000020_T003</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>43761</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transcript:Zm00001eb000050_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>108553</td>\n",
       "      <td>109927</td>\n",
       "      <td>ATGCCATTTCATGAGGTCGTCCAACAACCTCATAAGACGTTTGTAG...</td>\n",
       "      <td>Zm00001d027234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transcript:Zm00001eb000060_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>188922</td>\n",
       "      <td>189282</td>\n",
       "      <td>ATGGGGCTGAAGCGGCCTACGGCGGGAGCGGGGGAGGCAGCAGCGC...</td>\n",
       "      <td>Zm00001d027239</td>\n",
       "      <td>9.589635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transcript:Zm00001eb000070_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>190191</td>\n",
       "      <td>198832</td>\n",
       "      <td>ATGAAGAGAAAGGAAAACTCCGCGCACTCGGCGTCGCCTCTCAACA...</td>\n",
       "      <td>Zm00001d027240</td>\n",
       "      <td>0.269903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transcript:Zm00001eb000080_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>201176</td>\n",
       "      <td>202121</td>\n",
       "      <td>ATGAAGACAGGGATCGTTGAGAGTGATCTGGTTCTAACAGTTAGTC...</td>\n",
       "      <td>Zm00001d027242</td>\n",
       "      <td>3.296181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transcript:Zm00001eb000100_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>207555</td>\n",
       "      <td>209641</td>\n",
       "      <td>ATGCTTCACTCCCAGTATGCCTCGCAGACGTCCAAACAGAACCCGG...</td>\n",
       "      <td>Zm00001d027244</td>\n",
       "      <td>0.061070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transcript:Zm00001eb000110_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>246464</td>\n",
       "      <td>247227</td>\n",
       "      <td>ATGGAAGGTGAATCATCTATTGTTGCTTCACCATTTCCTGTACCAA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transcript:Zm00001eb000120_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>315218</td>\n",
       "      <td>315578</td>\n",
       "      <td>CTGAACCATCGAATCCTAGCGATACGATTAGCTCCTCCCGTCCCCT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>transcript:Zm00001eb000140_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>325306</td>\n",
       "      <td>326897</td>\n",
       "      <td>ATGGGGAACGGCACGACCGTGGGCTCTGGCCTCGCCTCCACTGTGC...</td>\n",
       "      <td>Zm00001d027249</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>transcript:Zm00001eb000150_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>328213</td>\n",
       "      <td>328489</td>\n",
       "      <td>ATGCTCCTCCGTGCCCTGATCTTCCCGGACGACCTCCTCCCGCTCC...</td>\n",
       "      <td>Zm00001d027250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>transcript:Zm00001eb000170_T003</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>552259</td>\n",
       "      <td>564691</td>\n",
       "      <td>ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...</td>\n",
       "      <td>Zm00001d027257</td>\n",
       "      <td>1.982075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>transcript:Zm00001eb000170_T002</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>552259</td>\n",
       "      <td>564691</td>\n",
       "      <td>ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...</td>\n",
       "      <td>Zm00001d027257</td>\n",
       "      <td>1.982075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>transcript:Zm00001eb000170_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>552259</td>\n",
       "      <td>564691</td>\n",
       "      <td>ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...</td>\n",
       "      <td>Zm00001d027257</td>\n",
       "      <td>1.982075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>transcript:Zm00001eb000170_T004</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>552418</td>\n",
       "      <td>564691</td>\n",
       "      <td>ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...</td>\n",
       "      <td>Zm00001d027257</td>\n",
       "      <td>1.982075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>transcript:Zm00001eb000180_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>551123</td>\n",
       "      <td>552056</td>\n",
       "      <td>ATGTGCAGGTGTAGCTCCCTTCCGTCCTTGTCCCTGCCATCCCCGA...</td>\n",
       "      <td>Zm00001d027256</td>\n",
       "      <td>1.019810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>transcript:Zm00001eb000200_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>680018</td>\n",
       "      <td>680498</td>\n",
       "      <td>ATGGCCACTGCTCATCATGCGGATGACCATCGTGCAGGGAGAAGAA...</td>\n",
       "      <td>Zm00001d027258</td>\n",
       "      <td>3.564590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      transcript_id  chrom strand   start     end  \\\n",
       "0   transcript:Zm00001eb000010_T001      1      +   34721   38366   \n",
       "1   transcript:Zm00001eb000020_T004      1      -   41526   43900   \n",
       "2   transcript:Zm00001eb000020_T001      1      -   41526   45913   \n",
       "3   transcript:Zm00001eb000020_T002      1      -   42239   45913   \n",
       "4   transcript:Zm00001eb000020_T003      1      -   43761   45913   \n",
       "5   transcript:Zm00001eb000050_T001      1      -  108553  109927   \n",
       "6   transcript:Zm00001eb000060_T001      1      -  188922  189282   \n",
       "7   transcript:Zm00001eb000070_T001      1      -  190191  198832   \n",
       "8   transcript:Zm00001eb000080_T001      1      -  201176  202121   \n",
       "9   transcript:Zm00001eb000100_T001      1      -  207555  209641   \n",
       "10  transcript:Zm00001eb000110_T001      1      -  246464  247227   \n",
       "11  transcript:Zm00001eb000120_T001      1      +  315218  315578   \n",
       "12  transcript:Zm00001eb000140_T001      1      -  325306  326897   \n",
       "13  transcript:Zm00001eb000150_T001      1      +  328213  328489   \n",
       "14  transcript:Zm00001eb000170_T003      1      -  552259  564691   \n",
       "15  transcript:Zm00001eb000170_T002      1      -  552259  564691   \n",
       "16  transcript:Zm00001eb000170_T001      1      -  552259  564691   \n",
       "17  transcript:Zm00001eb000170_T004      1      -  552418  564691   \n",
       "18  transcript:Zm00001eb000180_T001      1      +  551123  552056   \n",
       "19  transcript:Zm00001eb000200_T001      1      +  680018  680498   \n",
       "\n",
       "                                             sequence            d_id  \\\n",
       "0   ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...  Zm00001d027230   \n",
       "1   TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...  Zm00001d027231   \n",
       "2   ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231   \n",
       "3   ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231   \n",
       "4   ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231   \n",
       "5   ATGCCATTTCATGAGGTCGTCCAACAACCTCATAAGACGTTTGTAG...  Zm00001d027234   \n",
       "6   ATGGGGCTGAAGCGGCCTACGGCGGGAGCGGGGGAGGCAGCAGCGC...  Zm00001d027239   \n",
       "7   ATGAAGAGAAAGGAAAACTCCGCGCACTCGGCGTCGCCTCTCAACA...  Zm00001d027240   \n",
       "8   ATGAAGACAGGGATCGTTGAGAGTGATCTGGTTCTAACAGTTAGTC...  Zm00001d027242   \n",
       "9   ATGCTTCACTCCCAGTATGCCTCGCAGACGTCCAAACAGAACCCGG...  Zm00001d027244   \n",
       "10  ATGGAAGGTGAATCATCTATTGTTGCTTCACCATTTCCTGTACCAA...             NaN   \n",
       "11  CTGAACCATCGAATCCTAGCGATACGATTAGCTCCTCCCGTCCCCT...             NaN   \n",
       "12  ATGGGGAACGGCACGACCGTGGGCTCTGGCCTCGCCTCCACTGTGC...  Zm00001d027249   \n",
       "13  ATGCTCCTCCGTGCCCTGATCTTCCCGGACGACCTCCTCCCGCTCC...  Zm00001d027250   \n",
       "14  ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...  Zm00001d027257   \n",
       "15  ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...  Zm00001d027257   \n",
       "16  ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...  Zm00001d027257   \n",
       "17  ATGCCGTCGCGTTCGCCGCGGCAGTCGGCCCGACCCCCCCTCCCGA...  Zm00001d027257   \n",
       "18  ATGTGCAGGTGTAGCTCCCTTCCGTCCTTGTCCCTGCCATCCCCGA...  Zm00001d027256   \n",
       "19  ATGGCCACTGCTCATCATGCGGATGACCATCGTGCAGGGAGAAGAA...  Zm00001d027258   \n",
       "\n",
       "     avg_tpm  \n",
       "0   2.477070  \n",
       "1   7.861029  \n",
       "2   7.861029  \n",
       "3   7.861029  \n",
       "4   7.861029  \n",
       "5   0.000000  \n",
       "6   9.589635  \n",
       "7   0.269903  \n",
       "8   3.296181  \n",
       "9   0.061070  \n",
       "10       NaN  \n",
       "11       NaN  \n",
       "12  0.000000  \n",
       "13  0.000000  \n",
       "14  1.982075  \n",
       "15  1.982075  \n",
       "16  1.982075  \n",
       "17  1.982075  \n",
       "18  1.019810  \n",
       "19  3.564590  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dropna(subset=['avg_tpm'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb8f0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>chrom</th>\n",
       "      <th>strand</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>d_id</th>\n",
       "      <th>avg_tpm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript:Zm00001eb000010_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>+</td>\n",
       "      <td>34721</td>\n",
       "      <td>38366</td>\n",
       "      <td>ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...</td>\n",
       "      <td>Zm00001d027230</td>\n",
       "      <td>2.477070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcript:Zm00001eb000020_T004</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>43900</td>\n",
       "      <td>TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcript:Zm00001eb000020_T001</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>41526</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcript:Zm00001eb000020_T002</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>42239</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcript:Zm00001eb000020_T003</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>43761</td>\n",
       "      <td>45913</td>\n",
       "      <td>ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...</td>\n",
       "      <td>Zm00001d027231</td>\n",
       "      <td>7.861029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     transcript_id  chrom strand  start    end  \\\n",
       "0  transcript:Zm00001eb000010_T001      1      +  34721  38366   \n",
       "1  transcript:Zm00001eb000020_T004      1      -  41526  43900   \n",
       "2  transcript:Zm00001eb000020_T001      1      -  41526  45913   \n",
       "3  transcript:Zm00001eb000020_T002      1      -  42239  45913   \n",
       "4  transcript:Zm00001eb000020_T003      1      -  43761  45913   \n",
       "\n",
       "                                            sequence            d_id   avg_tpm  \n",
       "0  ATGGCCGCCGCCGCCGCCACTTTCGGCTTCCTCCATCCTCCAATCC...  Zm00001d027230  2.477070  \n",
       "1  TTATTTTACCTCTGTAGTTCTGTTTGTTGCTCTATCCCTTCCCGTT...  Zm00001d027231  7.861029  \n",
       "2  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  \n",
       "3  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  \n",
       "4  ATGGCTGAGAAGGTGAAGGAGAAGATGCTGATGCTCCGCAGCAGCG...  Zm00001d027231  7.861029  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5470aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of sequences: 16278\n"
     ]
    }
   ],
   "source": [
    "max_len=final_df['sequence'].str.len().max()\n",
    "print(\"Max length of sequences:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "965dff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"dataset/dataset1/final_matched_cds_tpm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14161980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chrom  start    end   avg_tpm  transcript_index  strand_numeric  \\\n",
      "0      1  34721  38366  2.477070                 0               1   \n",
      "1      1  41526  43900  7.861029                 1               0   \n",
      "2      1  41526  45913  7.861029                 2               0   \n",
      "3      1  42239  45913  7.861029                 3               0   \n",
      "4      1  43761  45913  7.861029                 4               0   \n",
      "\n",
      "                                    sequence_encoded  expression_label  \n",
      "0  [0, 1, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, ...                 1  \n",
      "1  [1, 1, 0, 1, 1, 1, 1, 0, 3, 3, 1, 3, 1, 2, 1, ...                 2  \n",
      "2  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  \n",
      "3  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  \n",
      "4  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Load your dataset\n",
    "df = pd.read_csv(\"dataset/dataset1/final_matched_cds_tpm.csv\")  # Replace with your actual file path\n",
    "\n",
    "# === 2. Encode `transcript_id` numerically\n",
    "df[\"transcript_index\"] = pd.factorize(df[\"transcript_id\"])[0]\n",
    "\n",
    "# === 3. Encode `strand`: '+' -> 1, '-' -> 0\n",
    "df[\"strand_numeric\"] = df[\"strand\"].map({'+': 1, '-': 0})\n",
    "\n",
    "# === 4. Encode `sequence`: A=0, T=1, G=2, C=3, others=4\n",
    "def encode_sequence(seq):\n",
    "    mapping = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
    "    return [mapping.get(base.upper(), 4) for base in seq]\n",
    "\n",
    "df[\"sequence_encoded\"] = df[\"sequence\"].apply(encode_sequence)\n",
    "\n",
    "# === 5. Compute expression label from avg_tpm\n",
    "def compute_labels(tpm_array):\n",
    "    mean_tpm = np.mean(tpm_array)\n",
    "    low = mean_tpm / 2\n",
    "    high = mean_tpm * 1.5\n",
    "    labels = []\n",
    "    for t in tpm_array:\n",
    "        if t < 1:\n",
    "            labels.append(0)\n",
    "        elif t > 6:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return np.array(labels, dtype=np.int32)\n",
    "\n",
    "df[\"expression_label\"] = compute_labels(df[\"avg_tpm\"].values)\n",
    "\n",
    "# === 6. Drop unnecessary columns\n",
    "df = df.drop(columns=[\"transcript_id\", \"strand\", \"sequence\", \"d_id\"])\n",
    "\n",
    "# === 7. Output final DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: save it\n",
    "# df.to_csv(\"processed_gene_expression.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "453f4abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 22530), (2, 20732), (0, 22390)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "Counter(df['expression_label']).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d36e05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['sequence_encoded'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e36e1e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55359, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "950548cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LEN = 6000\n",
    "PAD_VALUE = 4\n",
    "\n",
    "def pad_or_truncate(seq):\n",
    "    if len(seq) > FIXED_LEN:\n",
    "        return seq[:FIXED_LEN]\n",
    "    return seq + [PAD_VALUE] * (FIXED_LEN - len(seq))\n",
    "\n",
    "df['sequence_encoded'] = df['sequence_encoded'].apply(pad_or_truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b79b901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['avg_tpm'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4461099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcript_index</th>\n",
       "      <th>strand_numeric</th>\n",
       "      <th>sequence_encoded</th>\n",
       "      <th>expression_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34721</td>\n",
       "      <td>38366</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41526</td>\n",
       "      <td>43900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 1, 1, 1, 1, 0, 3, 3, 1, 3, 1, 2, 1, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>41526</td>\n",
       "      <td>45913</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>42239</td>\n",
       "      <td>45913</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>43761</td>\n",
       "      <td>45913</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom  start    end  transcript_index  strand_numeric  \\\n",
       "0      1  34721  38366                 0               1   \n",
       "1      1  41526  43900                 1               0   \n",
       "2      1  41526  45913                 2               0   \n",
       "3      1  42239  45913                 3               0   \n",
       "4      1  43761  45913                 4               0   \n",
       "\n",
       "                                    sequence_encoded  expression_label  \n",
       "0  [0, 1, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, ...                 1  \n",
       "1  [1, 1, 0, 1, 1, 1, 1, 0, 3, 3, 1, 3, 1, 2, 1, ...                 2  \n",
       "2  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  \n",
       "3  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  \n",
       "4  [0, 1, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, ...                 2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f400e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression_label\n",
      "0    19419\n",
      "1    18703\n",
      "2    17237\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"expression_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d828839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "695fcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sequence and label to arrays (now all sequences have equal length)\n",
    "sequences = np.array(df['sequence_encoded'].tolist(), dtype=np.int32)  # shape: (n_samples, 7000)\n",
    "labels = df['expression_label'].values.astype(np.int32)                # shape: (n_samples,)\n",
    "\n",
    "# Save sequences and labels\n",
    "np.save('dataset/dataset1/sequence_encoded.npy', sequences)\n",
    "np.save('dataset/dataset1/expression_label.npy', labels)\n",
    "\n",
    "# Optionally save both stacked together (if needed for specific use)\n",
    "seq_and_label = np.column_stack([sequences, labels])  # shape: (n_samples, 7001)\n",
    "np.save('dataset/dataset1/seq_and_label.npy', seq_and_label)\n",
    "\n",
    "# Save other numerical features\n",
    "other_features = df.drop(columns=['sequence_encoded', 'expression_label'])\n",
    "np.save('dataset/dataset1/other_features.npy', other_features.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf8f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b743793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load your data ===\n",
    "x = np.load('dataset/dataset1/seq_and_label.npy')[:, :-1]  # shape: (n, 7000)\n",
    "y = np.load('dataset/dataset1/seq_and_label.npy')[:, -1].astype(int)  # shape: (n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1a8bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small=x[:10000,:6000]\n",
    "y_small=y[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6311cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55359, 6000)\n",
      "(10000, 6000)\n",
      "(55359,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_small.shape)\n",
    "print(y.shape)\n",
    "print(y_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c843f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 3347 samples (33.47%)\n",
      "Class 2: 3232 samples (32.32%)\n",
      "Class 0: 3421 samples (34.21%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# y_small should be your labels array\n",
    "counter = Counter(y_small)\n",
    "total = sum(counter.values())\n",
    "for k, v in counter.items():\n",
    "    print(f\"Class {k}: {v} samples ({v / total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e7a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.45977011494252873, 1: 2.6666666666666665, 2: 2.2222222222222223}\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 200s 36s/step - loss: 1.3584 - accuracy: 0.2750 - val_loss: 0.9627 - val_accuracy: 0.7000\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 148s 30s/step - loss: 1.2523 - accuracy: 0.2750 - val_loss: 1.1147 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 168s 34s/step - loss: 1.4817 - accuracy: 0.3500 - val_loss: 1.1360 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 168s 33s/step - loss: 1.1622 - accuracy: 0.2750 - val_loss: 1.1078 - val_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 140s 29s/step - loss: 0.9329 - accuracy: 0.4750 - val_loss: 1.0877 - val_accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 141s 26s/step - loss: 1.2553 - accuracy: 0.3500 - val_loss: 1.0562 - val_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 139s 29s/step - loss: 1.3042 - accuracy: 0.2500 - val_loss: 1.1051 - val_accuracy: 0.3000\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 151s 30s/step - loss: 1.1340 - accuracy: 0.3000 - val_loss: 1.1232 - val_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 172s 35s/step - loss: 1.3012 - accuracy: 0.2500 - val_loss: 1.1622 - val_accuracy: 0.2000\n",
      "Epoch 10/20\n",
      "2/5 [===========>..................] - ETA: 1:10 - loss: 1.3948 - accuracy: 0.1875"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Split the data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_small, y_small, test_size=0.2, random_state=42, stratify=y_small\n",
    ")\n",
    "\n",
    "# === Get positional encoding ===\n",
    "def get_positional_encoding(seq_len, model_dim):\n",
    "    angle_rads = np.arange(seq_len)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(model_dim)[np.newaxis, :] // 2)) / np.float32(model_dim)\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# === Sinusoidal Time Embedding ===\n",
    "class SinusoidalEmbedding(layers.Layer):\n",
    "    def __init__(self, model_dim):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def call(self, x):\n",
    "        half_dim = self.model_dim // 2\n",
    "        freqs = tf.exp(tf.linspace(tf.math.log(1.0), tf.math.log(1000.0), half_dim))\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        angles = 2.0 * math.pi * x * freqs\n",
    "        return tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)[..., tf.newaxis]\n",
    "\n",
    "# === Transformer Block ===\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, model_dim, heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=heads, key_dim=model_dim // heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='gelu'),\n",
    "            layers.Dense(model_dim),\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x)\n",
    "        x = self.norm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x + self.dropout2(ffn_output, training=training))\n",
    "\n",
    "# === Diffusion Transformer Model ===\n",
    "class DiffusionTransformer(tf.keras.Model):\n",
    "    def __init__(self, seq_len=7000, model_dim=32, num_heads=2, ff_dim=64, depth=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=5, output_dim=model_dim)  # A=0, T=1, G=2, C=3, PAD=4\n",
    "        self.pos_encoding = get_positional_encoding(seq_len, model_dim)\n",
    "        self.time_emb = SinusoidalEmbedding(model_dim)\n",
    "\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(model_dim, num_heads, ff_dim, rate=dropout_rate)\n",
    "            for _ in range(depth)\n",
    "        ]\n",
    "\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.dropout_final = layers.Dropout(0.3) \n",
    "        self.output_dense = layers.Dense(3, activation='softmax')  # 3 classes\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise_var = tf.zeros((tf.shape(inputs)[0], 1))  # Dummy time embedding\n",
    "        x = self.embedding(inputs)\n",
    "        x += self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "        noise_emb = self.time_emb(noise_var)\n",
    "        noise_emb = tf.transpose(noise_emb, [0, 2, 1])\n",
    "        noise_emb = tf.tile(noise_emb, [1, tf.shape(x)[1], 1])\n",
    "        x += tf.cast(noise_emb, tf.float32)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout_final(x, training=training)\n",
    "        return self.output_dense(x)\n",
    "\n",
    "# === Create and compile model ===\n",
    "model = DiffusionTransformer()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "]\n",
    "# === Train ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=8,\n",
    "    epochs=20,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# === Evaluate ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8f016a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_feats = np.load(\"dataset/dataset1/other_features.npy\")[:50]\n",
    "node_features = other_feats\n",
    "def create_random_hypergraph(num_nodes, num_hyperedges, connection_prob=0.1):\n",
    "    G = np.random.rand(num_nodes, num_hyperedges) < connection_prob\n",
    "    return G.astype(np.float32)\n",
    "\n",
    "G = create_random_hypergraph(num_nodes=50, num_hyperedges=50)\n",
    "G = np.expand_dims(G, axis=0)  # shape (1, 50, 50)\n",
    "node_features = np.expand_dims(node_features, axis=0)  # shape (1, 50, 5)\n",
    "\n",
    "y_small=np.expand_dims(y_small,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2558783.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "Predicted class probabilities: [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# === HGNN Layer ===\n",
    "class HGNNConv(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = layers.Dense(output_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x, G):\n",
    "        x = self.linear(x)\n",
    "        G_T = tf.transpose(G, perm=[0, 2, 1])  # (batch, H, N)\n",
    "        x = tf.matmul(G_T, x)  # (batch, H, feat)\n",
    "        return x\n",
    "\n",
    "# === HGNN + Classifier ===\n",
    "class HGNNEmbedding(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim=16, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.hgc1 = HGNNConv(input_dim, hidden_dim)\n",
    "        self.hgc2 = HGNNConv(hidden_dim, hidden_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, G, training=False):\n",
    "        x = self.hgc1(x, G)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.hgc2(x, G)\n",
    "        return tf.reduce_mean(x, axis=1)  # (batch, hidden_dim)\n",
    "\n",
    "\n",
    "# === Build and compile model ===\n",
    "model = HGNNEmbedding(input_dim=node_features.shape[-1], hidden_dim=16)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# === Train ===\n",
    "# For graph-level classification, use a single label per graph\n",
    "# Here, we use the majority class in y_small as the graph label\n",
    "graph_label = np.argmax(np.bincount(y_small.flatten()))\n",
    "graph_label = np.array([graph_label])  # shape (1,)\n",
    "\n",
    "history = model.fit(\n",
    "    x=[node_features, G],\n",
    "    y=graph_label,\n",
    "    batch_size=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# === Predict ===\n",
    "pred = model.predict([node_features, G])\n",
    "print(\"Predicted class probabilities:\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a89e787a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 40, 1, 1\n  y sizes: 40\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 181\u001b[0m\n\u001b[0;32m    177\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# === Train ===\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# === Train ===\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mcombined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_seq_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    186\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# === Evaluate on test data ===\u001b[39;00m\n\u001b[0;32m    189\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    190\u001b[0m     [X_seq_test, node_test, G],  \u001b[38;5;66;03m# DNA, node features, hypergraph\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     y_test,\n\u001b[0;32m    192\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1842\u001b[0m         label,\n\u001b[0;32m   1843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1844\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1845\u001b[0m         ),\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 40, 1, 1\n  y sizes: 40\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "# === Load Prepared Inputs ===\n",
    "sequence_and_labels = np.load('dataset/dataset1/seq_and_label.npy')[:50]  # (200, 6001)\n",
    "other_features = np.load('dataset/dataset1/other_features.npy')[:50]      # (200, 5)\n",
    "\n",
    "# === Split Inputs ===\n",
    "dna_sequences = sequence_and_labels[:, :-1]         # (200, 6000)\n",
    "expression_labels = sequence_and_labels[:, -1]      # (200,)\n",
    "node_features = other_features                      # (200, 5)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "        \n",
    "# Split into train and test sets\n",
    "X_seq_train, X_seq_test, y_train, y_test, node_train, node_test = train_test_split(\n",
    "    dna_sequences,\n",
    "    expression_labels,\n",
    "    node_features,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=expression_labels  # Maintain label balance\n",
    ")\n",
    "\n",
    "\n",
    "# Expand dimensions for graph input\n",
    "G = np.expand_dims(G, axis=0)                 # (1, 50, 50)\n",
    "node_train = np.expand_dims(node_train, axis=0)  # (1, 40, 5)\n",
    "node_test = np.expand_dims(node_test, axis=0)    # (1, 10, 5)\n",
    "\n",
    "\n",
    "# === Create Dummy Hypergraph ===\n",
    "def create_random_hypergraph(num_nodes, num_hyperedges, connection_prob=0.1):\n",
    "    G = np.random.rand(num_nodes, num_hyperedges) < connection_prob\n",
    "    return G.astype(np.float32)\n",
    "\n",
    "G = create_random_hypergraph(num_nodes=50, num_hyperedges=50)\n",
    "G = np.expand_dims(G, axis=0)                      # (1, 50, 50)\n",
    "node_features = np.expand_dims(node_features, 0)   # (1, 50, 5)\n",
    "dna_sequences = dna_sequences                      # (50, 6000)\n",
    "expression_labels = np.expand_dims(expression_labels,axis=1)              # (50,)\n",
    "\n",
    "# === Define Sinusoidal Embedding and TransformerBlock ===\n",
    "class SinusoidalEmbedding(layers.Layer):\n",
    "    def __init__(self, model_dim):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def call(self, x):\n",
    "        half_dim = self.model_dim // 2\n",
    "        freqs = tf.exp(tf.linspace(tf.math.log(1.0), tf.math.log(1000.0), half_dim))\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        angles = 2.0 * np.pi * x * freqs\n",
    "        return tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)[..., tf.newaxis]\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, model_dim, heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=heads, key_dim=model_dim // heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='gelu'),\n",
    "            layers.Dense(model_dim),\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x)\n",
    "        x = self.norm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x + self.dropout2(ffn_output, training=training))\n",
    "\n",
    "# === Positional Encoding ===\n",
    "def get_positional_encoding(seq_len, model_dim):\n",
    "    angle_rads = np.arange(seq_len)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(model_dim)[np.newaxis, :] // 2)) / np.float32(model_dim)\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# === Diffusion Transformer ===\n",
    "class DiffusionTransformer(tf.keras.Model):\n",
    "    def __init__(self, seq_len=6000, model_dim=128, num_heads=4, ff_dim=256, depth=4, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=5, output_dim=model_dim)\n",
    "        self.pos_encoding = get_positional_encoding(seq_len, model_dim)\n",
    "        self.time_emb = SinusoidalEmbedding(model_dim)\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(model_dim, num_heads, ff_dim, rate=dropout_rate)\n",
    "            for _ in range(depth)\n",
    "        ]\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise_var = tf.zeros((tf.shape(inputs)[0], 1))  # Dummy timestep\n",
    "        x = self.embedding(inputs)\n",
    "        x += self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "        noise_emb = self.time_emb(noise_var)\n",
    "        noise_emb = tf.transpose(noise_emb, [0, 2, 1])\n",
    "        noise_emb = tf.tile(noise_emb, [1, tf.shape(x)[1], 1])\n",
    "        x += tf.cast(noise_emb, tf.float32)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        return self.global_pool(x)\n",
    "\n",
    "# === HGNN Block ===\n",
    "class HGNNConv(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = layers.Dense(output_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x, G):\n",
    "        x = self.linear(x)\n",
    "        G_T = tf.transpose(G, perm=[0, 2, 1])\n",
    "        x = tf.matmul(G_T, x)\n",
    "        return x\n",
    "\n",
    "class HGNNEmbedding(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim=64, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.hgc1 = HGNNConv(input_dim, hidden_dim)\n",
    "        self.hgc2 = HGNNConv(hidden_dim, hidden_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, G, training=False):\n",
    "        x = self.hgc1(x, G)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.hgc2(x, G)\n",
    "        return tf.reduce_mean(x, axis=1)\n",
    "\n",
    "# === Fusion Network ===\n",
    "class HybridAttentionFusion(tf.keras.Model):\n",
    "    def __init__(self, fusion_dim=64, num_classes=3, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.dense1 = layers.Dense(fusion_dim, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dense2 = layers.Dense(fusion_dim // 2, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# === Combined Model ===\n",
    "class CombinedModel(tf.keras.Model):\n",
    "    def __init__(self, diffusion_transformer, hgnn_embedding, fusion_dim=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.diffusion_transformer = diffusion_transformer\n",
    "        self.hgnn_embedding = hgnn_embedding\n",
    "        self.hybrid_fusion = HybridAttentionFusion(fusion_dim=fusion_dim, num_classes=num_classes)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        dna_seq, node_features, hg_adj = inputs\n",
    "        diffusion_out = self.diffusion_transformer(dna_seq, training=training)  # (batch, dim)\n",
    "        hgnn_out = self.hgnn_embedding(node_features, hg_adj, training=training)  # (batch, dim)\n",
    "        combined = tf.concat([diffusion_out, hgnn_out], axis=1)  # (batch, dim*2)\n",
    "        return self.hybrid_fusion(combined, training=training)\n",
    "\n",
    "# === Instantiate Models ===\n",
    "diffusion_transformer = DiffusionTransformer(seq_len=6000, model_dim=128, num_heads=4, ff_dim=256, depth=4)\n",
    "hgnn_embedding = HGNNEmbedding(input_dim=node_features.shape[-1], hidden_dim=64)\n",
    "combined_model = CombinedModel(diffusion_transformer, hgnn_embedding)\n",
    "\n",
    "# === Compile ===\n",
    "combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train ===\n",
    "# === Train ===\n",
    "combined_model.fit(\n",
    "    x=[X_seq_train, node_train, G],\n",
    "    y=y_train,\n",
    "    batch_size=8,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# === Evaluate on test data ===\n",
    "loss, accuracy = combined_model.evaluate(\n",
    "    [X_seq_test, node_test, G],  # DNA, node features, hypergraph\n",
    "    y_test,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65897b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values in y_train: [0 1 2]\n",
      "Unique label values in y_test: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique label values in y_train:\", np.unique(y_train))\n",
    "print(\"Unique label values in y_test:\", np.unique(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
